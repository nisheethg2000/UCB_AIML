{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What drives the price of a car?\n",
    "\n",
    "![](images/kurt.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OVERVIEW**\n",
    "\n",
    "In this application, you will explore a dataset from Kaggle. The original dataset contained information on 3 million used cars. The provided dataset contains information on 426K cars to ensure speed of processing.  Your goal is to understand what factors make a car more or less expensive.  As a result of your analysis, you should provide clear recommendations to your client -- a used car dealership -- as to what consumers value in a used car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CRISP-DM Framework\n",
    "\n",
    "<center>\n",
    "    <img src = images/crisp.png width = 50%/>\n",
    "</center>\n",
    "\n",
    "\n",
    "To frame the task, throughout our practical applications, we will refer back to a standard process in industry for data projects called CRISP-DM.  This process provides a framework for working through a data problem.  Your first step in this application will be to read through a brief overview of CRISP-DM [here](https://mo-pcco.s3.us-east-1.amazonaws.com/BH-PCMLAI/module_11/readings_starter.zip).  After reading the overview, answer the questions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding\n",
    "\n",
    "From a business perspective, we are tasked with identifying key drivers for used car prices.  In the CRISP-DM overview, we are asked to convert this business framing to a data problem definition.  Using a few sentences, reframe the task as a data task with the appropriate technical vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Objective\n",
    " Background: The used car business in America is a surprisingly complex ecosystem. Several interconnected factors contribute to this complexity, making it a challenging industry to navigate for both buyers and sellers for e.g. Variety of products, information asymetry between buyer and seller, fluctuating market dynamics etc.\n",
    " Here, we are trying to create an AI model to simplify the process of identifying which factors influence the used car prices more to help dealers take necessary action to maxizmize their sale. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Business Success Criteria\n",
    "Perform predictive analysis to provide recmmendaton to used car dealers on various ways they can maximize their sales. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recources: Enough historic data, data with features influencing car prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Understanding\n",
    "\n",
    "After considering the business understanding, we want to get familiar with our data.  Write down some steps that you would take to get to know the dataset and identify any quality issues within.  Take time to get to know the dataset and explore what information it contains and how this could be used to inform your business understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Identify and ignore nulls, \n",
    "-  normalize outliers, convert categorical features to numeric, \n",
    "-  Observe the dataset for interesting details and/or trends(are their estimates), \n",
    "-  scale the data to normalize magnitude of different features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "After our initial exploration and fine-tuning of the business understanding, it is time to construct our final dataset prior to modeling.  Here, we want to make sure to handle any integrity issues and cleaning, the engineering of new features, any transformations that we believe should happen (scaling, logarithms, normalization, etc.), and general preparation for modeling with `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from warnings import filterwarnings \n",
    "filterwarnings('ignore')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import plotly.express as px\n",
    "from scipy.linalg import svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data = pd.read_csv('data/vehicles.zip', compression = 'zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(car_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see how many rows have null in most of the columns except id, price, state and region\n",
    "car_data[car_data.isna().sum(axis=1) >= 14].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counting number of rows with price=0\n",
    "car_data[car_data['price'] == 0 ].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dropping rows with null values in most of the features.\n",
    "car_data = car_data[car_data.isna().sum(axis=1) < 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## checling to see what percent of values are null in each feature\n",
    "for car in car_data.columns:\n",
    "    print(f\"{car}:unique:{((car_data[car].nunique()/car_data[car].size)*100)}%  NotNull:{((car_data[car].count()/car_data[car].size)*100)}%  Null:{((car_data[car].isna().sum()/car_data[car].size)*100)}%:{car_data[car].dtype}\")\n",
    "print(\"total columns:\",car_data.columns.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping unnecessary columns\n",
    "\n",
    "- `ID` is a unique identifier for each customer but not continuous so cannot be used to set as an index and also not useful for PCA.\n",
    "- `VIN`  is not useful for PCA.\n",
    "- `condition`,`drive`,`paint_color` are not useful as it is not populated for more then 25% of the data.\n",
    "- `cylinders` is not useful as it is not populated for about 40% of the data.\n",
    "- `Region` is not as useful since we have `state`.\n",
    "- `size` is not useful as it is not populated for about 72% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_features = ['id','VIN','condition','cylinders','size','drive','paint_color','region']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_clean = car_data.drop(dropped_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking for outliers\n",
    "sns.boxplot(data=car_data_clean, x=\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_iqr(df, column):\n",
    "    \"\"\"\n",
    "    Identifies and removes outliers from a Pandas DataFrame column using the IQR method.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        column (str): The name of the column to check for outliers.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with outliers removed.\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    print(\"lower_bound\",lower_bound)\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    print(\"upper_bound\",upper_bound)\n",
    "    \n",
    "    filtered_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_clean = remove_outliers_iqr(car_data_clean, 'price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking for outliers\n",
    "sns.boxplot(data=car_data_clean, x=\"price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_clean['price'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "car_data_clean['model']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding and scaling the data to run PCA and determine correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_encode = car_data_clean.drop('price',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for cat in categorical_columns:\n",
    "m_estimator = ce.MEstimateEncoder(cols=car_data_encode.columns)\n",
    "car_data_encoded = m_estimator.fit_transform(car_data_encode, car_data_clean['price'])\n",
    "#X_test_encoded = m_estimator.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ_enc = ce.TargetEncoder(cols=car_data_encode.columns)\n",
    "car_data_encoded = targ_enc.fit_transform(car_data_encode, car_data_clean['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_data_encoded['price'] = car_data_clean['price']\n",
    "print(car_data_encoded.shape)\n",
    "print(type(car_data_encoded))\n",
    "print(car_data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining the Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature & target\n",
    "target = car_data_clean['price']\n",
    "#features = car_data.drop('price',axis=1)\n",
    "#scale data\n",
    "scaler=StandardScaler()\n",
    "car_data_encoded[car_data_encoded.columns]=scaler.fit_transform(car_data_encoded[car_data_encoded.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highest_corr = car_data_encoded.corr()[['price']].nlargest(columns = 'price', n = 2).index[1]\n",
    "\n",
    "print(\"highest correlation:\",highest_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = car_data_encoded.corr()\n",
    "plt.figure(figsize=(10, 8)) # Adjust size as needed\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(car_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot in 3D with Matplotlib\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "scatter = ax.scatter(X_pca_3d[:, 0], X_pca_3d[:, 1], X_pca_3d[:, 2], c=target, cmap='viridis', edgecolor='k')\n",
    "ax.set_xlabel('Principal Component 1')\n",
    "ax.set_ylabel('Principal Component 2')\n",
    "ax.set_zlabel('Principal Component 3')\n",
    "plt.title('3D PCA')\n",
    "legend1 = ax.legend(*scatter.legend_elements(), title='Price')\n",
    "ax.add_artist(legend1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pca to 2 dimensions\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(car_data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot in 2D with Matplotlib\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=target, cmap='viridis', edgecolor='k', s=25)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('2D PCA')\n",
    "plt.colorbar(label='Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#px.scatter(data_frame=car_data, x='price', y='year')\n",
    "variance = car_data_encoded.var()\n",
    "#high_variance_features = variance[variance > 10] \n",
    "print(variance)\n",
    "#X_train_encoded.boxplot(column=high_variance_features.index) \n",
    "#plt.boxplot(variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "With your (almost?) final dataset in hand, it is now time to build some models.  Here, you should build a number of different regression models with the price as the target.  In building your models, you should explore different parameters and be sure to cross-validate your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `M-Estimate Encoder`, `CatBoost Encoder` , `James-stein Encoder` will be used to encoding categorical columns into numerical. \n",
    "- Then will use LinearRegression and Ridge to perform predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'price'\n",
    "car_data_clean = car_data_clean.fillna('missing')\n",
    "X = car_data_clean.drop('price', axis=1)\n",
    "y = car_data_encoded['price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "ml_estimator = ce.MEstimateEncoder(cols=X_train.columns)\n",
    "mestimator_linear_pipeline = Pipeline([\n",
    "    ('mtransformer', ml_estimator), \n",
    "    ('mscalor',StandardScaler()),\n",
    "    ('mlinreg', LinearRegression())])\n",
    "mestimator_linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "mr_estimator = ce.MEstimateEncoder(cols=X_train.columns)\n",
    "mestimator_ridge_pipeline = Pipeline([\n",
    "    ('mrtransformer', mr_estimator), \n",
    "    ('mrscalor',StandardScaler()),\n",
    "    ('mridge', Ridge(alpha=1000))])\n",
    "mestimator_ridge_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "js_estimator = ce.JamesSteinEncoder(cols=X_train.columns)\n",
    "js_linear_pipeline = Pipeline([\n",
    "    ('jtransformer', js_estimator), \n",
    "    ('jscalor',StandardScaler()),\n",
    "    ('jlinreg', LinearRegression())])\n",
    "js_linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "jsr_estimator = ce.JamesSteinEncoder(cols=X_train.columns)\n",
    "js_ridge_pipeline = Pipeline([\n",
    "    ('jrtransformer', jsr_estimator), \n",
    "    ('jrscalor',StandardScaler()),\n",
    "    ('jridge', Ridge())])\n",
    "js_ridge_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "c_estimator = ce.CatBoostEncoder(cols=X_train.columns)\n",
    "c_linear_pipeline = Pipeline([\n",
    "    ('ctransformer', c_estimator), \n",
    "    ('cscalor',StandardScaler()),\n",
    "    ('clinreg', LinearRegression())])\n",
    "c_linear_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "cr_estimator = ce.CatBoostEncoder(cols=X_train.columns)\n",
    "cr_ridge_pipeline = Pipeline([\n",
    "    ('crtransformer', cr_estimator), \n",
    "    ('crscalor',StandardScaler()),\n",
    "    ('cridge', Ridge(alpha=1000))])\n",
    "cr_ridge_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = pd.DataFrame(columns=['loss','MEstimator_Linear','MEstimator_Ridge','JStein_Linear','JStein_Ridge','CBoost_Linear','CBoost_Ridge'])\n",
    "model_results['loss']=['MSE_Train','MSE_Test','MAE_Train','MAE_Test','R2_Train','R2_Test']\n",
    "model_results = model_results.set_index('loss')\n",
    "model_results.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with MEstimator Encoder, Standard SCaler and Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mestimator_linear_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = mestimator_linear_pipeline.predict(X_train)\n",
    "y_test_pred = mestimator_linear_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['MEstimator_Linear']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with MEstimator Encoder, Standard SCaler and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mestimator_ridge_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = mestimator_ridge_pipeline.predict(X_train)\n",
    "y_test_pred = mestimator_ridge_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['MEstimator_Ridge']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with JamesStein Encoder, Standard SCaler and Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_linear_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = js_linear_pipeline.predict(X_train)\n",
    "y_test_pred = js_linear_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['JStein_Linear']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with JamesStein Encoder, Standard SCaler and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "js_ridge_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = js_ridge_pipeline.predict(X_train)\n",
    "y_test_pred = js_ridge_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['JStein_Ridge']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with Cboost Encoder, Standard SCaler and Liner Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_linear_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = c_linear_pipeline.predict(X_train)\n",
    "y_test_pred = c_linear_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['CBoost_Linear']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model with Cboost Encoder, Standard SCaler and Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_ridge_pipeline.fit(X_train,y_train)\n",
    "y_train_pred = cr_ridge_pipeline.predict(X_train)\n",
    "y_test_pred = cr_ridge_pipeline.predict(X_test)\n",
    "train_mse = float(mean_squared_error(y_train,y_train_pred))\n",
    "test_mse = float(mean_squared_error(y_test,y_test_pred))\n",
    "train_mae = float(mean_absolute_error(y_train,y_train_pred))\n",
    "test_mae = float(mean_absolute_error(y_test,y_test_pred))\n",
    "\n",
    "# Compute R² using Scikit-Learn\n",
    "R2_test = r2_score(y_test, y_test_pred)\n",
    "R2_train = r2_score(y_train, y_train_pred)\n",
    "model_results['CBoost_Ridge']=[train_mse,test_mse,train_mae,test_mae,R2_test,R2_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "With some modeling accomplished, we aim to reflect on what we identify as a high-quality model and what we are able to learn from this.  We should review our business objective and explore how well we can provide meaningful insight into drivers of used car prices.  Your goal now is to distill your findings and determine whether the earlier phases need revisitation and adjustment or if you have information of value to bring back to your client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the permutation importance\n",
    "\n",
    "results = permutation_importance(mestimator_ridge_pipeline, X_test, y_test,n_repeats=10)\n",
    "#importances = pd.DataFrame(data=results.importances_mean, index=X.columns, columns=['Importance']).sort_values(by='Importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results['importances'])\n",
    "df = df.T\n",
    "df.columns = X_test.columns\n",
    "px.box(data_frame=df, orientation='h', title = 'Feature importance for price prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#for cat in categorical_columns:\n",
    "mr_estimator = ce.MEstimateEncoder(cols=X_train.columns)\n",
    "X_train_encoded =  mr_estimator.fit_transform(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parameters_for_given_alpha(alpha):\n",
    "    lm_with_ridge_model = Ridge(alpha = alpha)\n",
    "    lm_with_ridge_model.fit(X_train_encoded,y_train)\n",
    "    training_mse = mean_squared_error(lm_with_ridge_model.predict(X_train_encoded),y_train)\n",
    "    return alpha, *lm_with_ridge_model.coef_, training_mse\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_df = pd.DataFrame([get_parameters_for_given_alpha(alpha) for alpha in [0.01, 0.1, 1, 10, 100, 1000, 10000,100000]],\n",
    "                        columns = [\"alpha\", *X_train_encoded.columns,\"Training MSE\"])\n",
    "param_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(param_df, x = \"alpha\", y = \"Training MSE\", log_x = True, markers = True)\n",
    "#fig.write_image(\"MSE_vs_alpha_most_basic.png\", scale = 3)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'fit_intercept': [False, True]}\n",
    "\n",
    "lr_model_finder = GridSearchCV(LinearRegression(),\n",
    "                               parameters,\n",
    "                               scoring = \"neg_mean_squared_error\",\n",
    "                               cv=3)\n",
    "\n",
    "lr_model_finder.fit(X_train_encoded, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model_finder.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the different models and encoders combinations giving pretty consistent results which a R2 Score of ranging from 70% to 78%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of all the features Odometer, Model and Year are the most significant and have the most correlation with the Price of the car as can be seen in the Most important feature box chart above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment\n",
    "\n",
    "Now that we've settled on our models and findings, it is time to deliver the information to the client.  You should organize your work as a basic report that details your primary findings.  Keep in mind that your audience is a group of used car dealers interested in fine-tuning their inventory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The box chart of the most important features shows a very significant relationship between Odometer, model, year and Price of car sales. Customers are ready to pay a good price for cars which are not driven too much and are relatively newer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
