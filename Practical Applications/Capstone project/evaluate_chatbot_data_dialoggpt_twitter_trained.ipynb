{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Customer Service Chatbot with DialogGPT for Conversational Intents\n",
        "\n",
        "This Jupyter Notebook updates `evaluate_chatbot_data_dialoggpt_twitter_trained.ipynb` to fix syntax errors and a class mismatch error in model evaluation (5 classes vs. 11 target names). It uses Microsoft’s DialogGPT for conversational intents (greeting, farewell, small_talk, compliment, weather_query) and DistilBERT for trucking-specific intents (delivery_status, billing_issue, account_update, service_inquiry, fuel_card_query, general_query). Trains on Twitter dataset (`tweets.csv` or `trucking_chatbot_test_dataset.csv`) with interactive ipywidgets UI.\n",
        "\n",
        "## Objectives\n",
        "- Inspect Twitter dataset for intents and entities (e.g., location, company).\n",
        "- Train DistilBERT for intent classification.\n",
        "- Fine-tune DialogGPT on conversational Twitter data.\n",
        "- Evaluate DistilBERT with accuracy, F1-score, confusion matrix, and dialogue success rate.\n",
        "- Implement hybrid dialogue management with DialogGPT and DistilBERT.\n",
        "- Provide interactive UI for customer interaction.\n",
        "\n",
        "## Requirements\n",
        "- Python 3.8 (recommended; 3.9 also compatible)\n",
        "- Install: `pip install transformers==4.44.2 torch==2.5.0 pandas==2.2.3 numpy==2.1.1 scikit-learn==1.5.2 datasets==3.0.1 seaborn==0.13.2 matplotlib==3.9.2 ipywidgets==8.1.5`\n",
        "- For GPU: `pip install torch==2.5.0+cu121 --index-url https://download.pytorch.org/whl/cu121`\n",
        "- Place `trucking_chatbot_test_dataset.csv` or `tweets.csv` in the directory.\n",
        "- Enable widgets: `jupyter nbextension enable --py widgetsnbextension`\n",
        "\n",
        "## Notes\n",
        "- Dataset: https://www.kaggle.com/thoughtvector/customer-support-on-twitter\n",
        "- Reflects tariffs/Moody’s downgrade in billing/fuel inquiries.\n",
        "- Professional responses with dynamic DialogGPT conversation.\n",
        "- Date: May 29, 2025, 1:44 PM EDT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: transformers==4.44.2 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (4.44.2)\n",
            "Requirement already satisfied: torch==2.6.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (2.6.0)\n",
            "Requirement already satisfied: accelerate==0.21.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (0.21.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (2.2.2)\n",
            "Requirement already satisfied: numpy==1.25.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (1.25.0)\n",
            "Requirement already satisfied: scikit-learn==1.5.2 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (1.5.2)\n",
            "Requirement already satisfied: datasets==3.0.1 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (3.0.1)\n",
            "Requirement already satisfied: seaborn==0.13.2 in c:\\programdata\\anaconda3\\lib\\site-packages (0.13.2)\n",
            "Requirement already satisfied: matplotlib==3.9.2 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (3.9.2)\n",
            "Requirement already satisfied: ipywidgets==8.1.5 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (8.1.5)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.44.2) (0.32.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.44.2) (0.5.3)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from transformers==4.44.2) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers==4.44.2) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch==2.6.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.6.0) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from torch==2.6.0) (1.13.1)\n",
            "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate==0.21.0) (5.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.2.2) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas==2.2.2) (2023.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.2) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn==1.5.2) (3.5.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==3.0.1) (16.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==3.0.1) (0.3.8)\n",
            "Requirement already satisfied: xxhash in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from datasets==3.0.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from datasets==3.0.1) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in c:\\programdata\\anaconda3\\lib\\site-packages (from datasets==3.0.1) (3.11.10)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib==3.9.2) (3.2.0)\n",
            "Requirement already satisfied: comm>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==8.1.5) (0.2.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==8.1.5) (8.30.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipywidgets==8.1.5) (5.14.3)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.12 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets==8.1.5) (4.0.14)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from ipywidgets==8.1.5) (3.0.15)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch==2.6.0) (1.3.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->datasets==3.0.1) (1.18.0)\n",
            "Requirement already satisfied: idna>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==3.0.1) (3.7)\n",
            "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.19.2)\n",
            "Requirement already satisfied: matplotlib-inline in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.1.6)\n",
            "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (3.0.43)\n",
            "Requirement already satisfied: pygments>=2.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (2.19.1)\n",
            "Requirement already satisfied: stack-data in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.2.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from ipython>=6.1.0->ipywidgets==8.1.5) (0.4.6)\n",
            "Requirement already satisfied: wcwidth in c:\\programdata\\anaconda3\\lib\\site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets==8.1.5) (0.2.5)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets==8.1.5) (0.8.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.44.2) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.44.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers==4.44.2) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch==2.6.0) (3.0.2)\n",
            "Requirement already satisfied: executing in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.5) (0.8.3)\n",
            "Requirement already satisfied: asttokens in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.5) (2.0.5)\n",
            "Requirement already satisfied: pure-eval in c:\\programdata\\anaconda3\\lib\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets==8.1.5) (0.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.44.2 torch==2.6.0 pandas==2.2.2 numpy==1.25.0 scikit-learn==1.5.2 datasets==3.0.1 seaborn==0.13.2 matplotlib==3.9.2 ipywidgets==8.1.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: jaraco.functools in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.0)\n",
            "Collecting jaraco.functools\n",
            "  Downloading jaraco.functools-4.1.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: more-itertools in c:\\programdata\\anaconda3\\lib\\site-packages (from jaraco.functools) (10.3.0)\n",
            "Downloading jaraco.functools-4.1.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: jaraco.functools\n",
            "Successfully installed jaraco.functools-4.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install --no-cache-dir --upgrade jaraco.functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: accelerate in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (0.21.0)\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-1.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (1.25.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
            "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (0.32.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in c:\\users\\nishe\\appdata\\roaming\\python\\python311\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n",
            "Downloading accelerate-1.7.0-py3-none-any.whl (362 kB)\n",
            "Installing collected packages: accelerate\n",
            "  Attempting uninstall: accelerate\n",
            "    Found existing installation: accelerate 0.21.0\n",
            "    Uninstalling accelerate-0.21.0:\n",
            "      Successfully uninstalled accelerate-0.21.0\n",
            "Successfully installed accelerate-1.7.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The scripts accelerate-config.exe, accelerate-estimate-memory.exe, accelerate-launch.exe, accelerate-merge-weights.exe and accelerate.exe are installed in 'C:\\Users\\nishe\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "pip install --no-cache-dir --upgrade accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from datasets import Dataset, ClassLabel\n",
        "import torch\n",
        "import json\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import re\n",
        "from datetime import datetime\n",
        "import os\n",
        "from matplotlib.pyplot import text\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Inspect and Preprocess Dataset\n",
        "\n",
        "Load Twitter dataset, add conversational examples, label intents/entities. Ensure all intents are preserved.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Missing intents [] or low sample counts {'greeting': 36847, 'general_query': 7932, 'farewell': 4701, 'service_inquiry': 2235, 'weather_query': 1530, 'account_update': 1442, 'delivery_status': 1093, 'billing_issue': 837, 'compliment': 145, 'fuel_card_query': 76, 'small_talk': 2}. Adding placeholders.\n",
            "Dataset Shape: (56843, 9)\n",
            "Columns: ['tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'response_tweet_id', 'in_response_to_tweet_id', 'intent', 'entities']\n",
            "Sample Rows:\n",
            "    tweet_id   author_id inbound                      created_at  \\\n",
            "0       2.0      115712    True  Tue Oct 31 22:11:45 +0000 2017   \n",
            "1       6.0  sprintcare   False  Tue Oct 31 21:46:24 +0000 2017   \n",
            "2       8.0      115712    True  Tue Oct 31 21:45:10 +0000 2017   \n",
            "3      11.0  sprintcare   False  Tue Oct 31 22:10:35 +0000 2017   \n",
            "4      16.0      115713    True  Tue Oct 31 20:00:43 +0000 2017   \n",
            "\n",
            "                                                text response_tweet_id  \\\n",
            "0      @sprintcare and how do you propose we do that               NaN   \n",
            "1  @115712 Can you please send us a private messa...               5,7   \n",
            "2          @sprintcare is the worst customer service            9,6,10   \n",
            "3  @115713 This is saddening to hear. Please shoo...               NaN   \n",
            "4  @sprintcare Since I signed up with you....Sinc...                15   \n",
            "\n",
            "   in_response_to_tweet_id           intent entities  \n",
            "0                      1.0    general_query       []  \n",
            "1                      8.0    general_query       []  \n",
            "2                      NaN  service_inquiry       []  \n",
            "3                     12.0         greeting       []  \n",
            "4                     17.0    general_query       []  \n",
            "Missing Values:\n",
            " tweet_id                       3\n",
            "author_id                      3\n",
            "inbound                        3\n",
            "created_at                     3\n",
            "text                           0\n",
            "response_tweet_id          18293\n",
            "in_response_to_tweet_id    15727\n",
            "intent                         0\n",
            "entities                       0\n",
            "dtype: int64\n",
            "Intent Distribution:\n",
            " intent\n",
            "greeting           36847\n",
            "general_query       7932\n",
            "farewell            4701\n",
            "service_inquiry     2235\n",
            "weather_query       1530\n",
            "account_update      1442\n",
            "delivery_status     1093\n",
            "billing_issue        837\n",
            "compliment           145\n",
            "fuel_card_query       76\n",
            "small_talk             5\n",
            "Name: count, dtype: int64\n",
            "Unique Intents: ['account_update', 'billing_issue', 'compliment', 'delivery_status', 'farewell', 'fuel_card_query', 'general_query', 'greeting', 'service_inquiry', 'small_talk', 'weather_query']\n",
            "Entity Samples:\n",
            "                                                     text  \\\n",
            "63     @ChipotleTweets name a better halloween duo #s...   \n",
            "34519  @ArgosHelpers Hi, any idea if you're going to ...   \n",
            "\n",
            "                                            entities  \n",
            "63     [{'entity': 'location', 'value': 'Speedway'}]  \n",
            "34519  [{'entity': 'location', 'value': 'Speedway'}]  \n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "def inspect_dataset(file_path='twcs_lessrecords.csv'):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "    except FileNotFoundError:\n",
        "        print('Dataset not found. Using Twitter dataset.')\n",
        "        try:\n",
        "            df = pd.read_csv('tweets.csv')\n",
        "        except FileNotFoundError:\n",
        "            print('Download from: https://www.kaggle.com/thoughtvector/customer-support-on-twitter')\n",
        "            data = {\n",
        "                'text': [\n",
        "                    'Where is my shipment from Speedway?',\n",
        "                    'Why is my Comdata bill so high?',\n",
        "                    'Need to update my address for IFTA',\n",
        "                    'What are your hauling rates?',\n",
        "                    'Help with my account',\n",
        "                    'Hello',\n",
        "                    'How are you',\n",
        "                    'How can I help you',\n",
        "                    'Track my cargo',\n",
        "                    'Overcharged on Comdata invoice',\n",
        "                    'Hi there',\n",
        "                    'Change my contact info',\n",
        "                    'Tell me about Comdata services',\n",
        "                    'Lost my shipment',\n",
        "                    'Good morning',\n",
        "                    'Thanks for your help',\n",
        "                    'Bye',\n",
        "                    'What’s new?',\n",
        "                    'How’s it going?',\n",
        "                    'How’s your day going?',\n",
        "                    'Any big plans?',\n",
        "                    'How’s the trucking life?',\n",
        "                    'You’re awesome!',\n",
        "                    'Great job!',\n",
        "                    'How’s the weather there?',\n",
        "                    'Is it raining?'\n",
        "                ],\n",
        "                'intent': [\n",
        "                    'delivery_status',\n",
        "                    'billing_issue',\n",
        "                    'account_update',\n",
        "                    'service_inquiry',\n",
        "                    'general_query',\n",
        "                    'greeting',\n",
        "                    'greeting',\n",
        "                    'greeting',\n",
        "                    'delivery_status',\n",
        "                    'billing_issue',\n",
        "                    'greeting',\n",
        "                    'account_update',\n",
        "                    'fuel_card_query',\n",
        "                    'general_query',\n",
        "                    'greeting',\n",
        "                    'farewell',\n",
        "                    'farewell',\n",
        "                    'small_talk',\n",
        "                    'small_talk',\n",
        "                    'small_talk',\n",
        "                    'small_talk',\n",
        "                    'small_talk',\n",
        "                    'compliment',\n",
        "                    'compliment',\n",
        "                    'weather_query',\n",
        "                    'weather_query'\n",
        "                ]\n",
        "            }\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "    # Define expected intents to ensure all are present\n",
        "    expected_intents = ['delivery_status', 'billing_issue', 'account_update', 'service_inquiry', 'fuel_card_query', 'general_query', 'greeting', 'farewell', 'small_talk', 'compliment', 'weather_query']\n",
        "\n",
        "    keywords = ['delivery', 'shipment', 'cargo', 'bill', 'invoice', 'payment', 'account', 'service', 'hauling', 'truck', 'comdata', 'ifta', 'speedway', 'fuel', 'tax', 'station', 'hello', 'hi', 'how', 'good', 'bye', 'thanks', 'new', 'day', 'plans', 'awesome', 'great', 'weather', 'rain']\n",
        "    if 'text' in df.columns:\n",
        "        df = df[df['text'].str.contains('|'.join(keywords), case=False, na=False)]\n",
        "        # Avoid sampling to prevent losing intents\n",
        "        # df = df.sample(n=min(10000, len(df)), random_state=42)  # Removed to preserve all intents\n",
        "\n",
        "    if 'intent' not in df.columns:\n",
        "        def label_intent(text):\n",
        "            text = text.lower()\n",
        "            if any(word in text for word in ['hello', 'hi', 'how are you', 'how can i help', 'good morning', 'good afternoon']):\n",
        "                return 'greeting'\n",
        "            elif any(word in text for word in ['goodbye', 'bye', 'thanks', 'thank you']):\n",
        "                return 'farewell'\n",
        "            elif any(word in text for word in ['what’s new', 'how’s it going', 'how’s your day', 'any big plans', 'trucking life']):\n",
        "                return 'small_talk'\n",
        "            elif any(word in text for word in ['awesome', 'great job', 'you rock']):\n",
        "                return 'compliment'\n",
        "            elif any(word in text for word in ['weather', 'rain', 'sunny']):\n",
        "                return 'weather_query'\n",
        "            elif any(word in text for word in ['delivery', 'shipment', 'track', 'cargo']):\n",
        "                return 'delivery_status'\n",
        "            elif any(word in text for word in ['bill', 'invoice', 'payment', 'charge']):\n",
        "                return 'billing_issue'\n",
        "            elif any(word in text for word in ['update', 'change', 'address', 'contact']):\n",
        "                return 'account_update'\n",
        "            elif any(word in text for word in ['service', 'rate', 'hauling']):\n",
        "                return 'service_inquiry'\n",
        "            elif any(word in text for word in ['comdata', 'ifta', 'fuel', 'tax']):\n",
        "                return 'fuel_card_query'\n",
        "            else:\n",
        "                return 'general_query'\n",
        "        df['intent'] = df['text'].apply(label_intent)\n",
        "\n",
        "    def extract_entities(text):\n",
        "        entities = []\n",
        "        text = text.lower()\n",
        "        if 'speedway' in text:\n",
        "            entities.append({'entity': 'location', 'value': 'Speedway'})\n",
        "        if 'comdata' in text:\n",
        "            entities.append({'entity': 'company', 'value': 'Comdata'})\n",
        "        if 'ifta' in text:\n",
        "            entities.append({'entity': 'regulation', 'value': 'IFTA'})\n",
        "        return entities\n",
        "\n",
        "    df['entities'] = df['text'].apply(extract_entities)\n",
        "\n",
        "   # Ensure all intents and sufficient samples\n",
        "    missing_intents = [intent for intent in expected_intents if intent not in df['intent'].unique()]\n",
        "    intent_counts = df['intent'].value_counts()\n",
        "    if missing_intents or any(count < 5 for count in intent_counts.get(expected_intents, [0])):\n",
        "        print(f'Warning: Missing intents {missing_intents} or low sample counts {intent_counts.to_dict()}. Adding placeholders.')\n",
        "        placeholder_data = []\n",
        "        for intent in expected_intents:\n",
        "            current_count = intent_counts.get(intent, 0)\n",
        "            if current_count < 5:\n",
        "                for _ in range(5 - current_count):\n",
        "                    if intent == 'compliment':\n",
        "                        placeholder_data.append({'text': f'You guys are {random.choice([\"great\", \"awesome\", \"fantastic\"])}!', 'intent': 'compliment', 'entities': []})\n",
        "                    elif intent == 'weather_query':\n",
        "                        placeholder_data.append({'text': f'Is it {random.choice([\"raining\", \"sunny\", \"snowing\"])} on my route?', 'intent': 'weather_query', 'entities': []})\n",
        "                    elif intent == 'farewell':\n",
        "                        placeholder_data.append({'text': f'{random.choice([\"Thanks, bye\", \"Goodbye\", \"See ya\"])}!', 'intent': 'farewell', 'entities': []})\n",
        "                    elif intent == 'small_talk':\n",
        "                        placeholder_data.append({'text': f'{random.choice([\"How’s the day going?\", \"What’s new with you?\", \"How’s trucking?\"])}', 'intent': 'small_talk', 'entities': []})\n",
        "                    elif intent == 'greeting':\n",
        "                        placeholder_data.append({'text': f'{random.choice([\"Hello\", \"Hi\", \"Good morning\"])}!', 'intent': 'greeting', 'entities': []})\n",
        "                    elif intent == 'delivery_status':\n",
        "                        placeholder_data.append({'text': f'Track my shipment {random.randint(100, 999)}', 'intent': 'delivery_status', 'entities': []})\n",
        "                    elif intent == 'billing_issue':\n",
        "                        placeholder_data.append({'text': f'Why is my bill ${random.randint(100, 1000)}?', 'intent': 'billing_issue', 'entities': []})\n",
        "                    elif intent == 'account_update':\n",
        "                        placeholder_data.append({'text': f'Update my {random.choice([\"address\", \"contact\", \"email\"])}', 'intent': 'account_update', 'entities': []})\n",
        "                    elif intent == 'service_inquiry':\n",
        "                        placeholder_data.append({'text': f'What are your {random.choice([\"rates\", \"services\", \"routes\"])}?', 'intent': 'service_inquiry', 'entities': []})\n",
        "                    elif intent == 'fuel_card_query':\n",
        "                        placeholder_data.append({'text': f'Issue with my {random.choice([\"Comdata card\", \"IFTA\", \"fuel account\"])}', 'intent': 'fuel_card_query', 'entities': [{'entity': 'company', 'value': 'Comdata'}] if 'comdata' in text.lower() else []})\n",
        "                    elif intent == 'general_query':\n",
        "                        placeholder_data.append({'text': f'Need help with {random.choice([\"something\", \"my account\", \"a question\"])}', 'intent': 'general_query', 'entities': []})\n",
        "        if placeholder_data:\n",
        "            df = pd.concat([df, pd.DataFrame(placeholder_data)], ignore_index=True)\n",
        "            \n",
        "    print('Dataset Shape:', df.shape)\n",
        "    print('Columns:', df.columns.tolist())\n",
        "    print('Sample Rows:\\n', df.head())\n",
        "    print('Missing Values:\\n', df.isnull().sum())\n",
        "    print('Intent Distribution:\\n', df['intent'].value_counts())\n",
        "    print('Unique Intents:', sorted(df['intent'].unique()))\n",
        "    print('Entity Samples:\\n', df[df['entities'].apply(len) > 0][['text', 'entities']].head())\n",
        "    return df\n",
        "\n",
        "df = inspect_dataset()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Prepare DialogGPT Training Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DialogGPT training data prepared: 43225 dialogue pairs.\n"
          ]
        }
      ],
      "source": [
        "def prepare_dialoggpt_training_data(df):\n",
        "    conversational_intents = ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query']\n",
        "    df_conversational = df[df['intent'].isin(conversational_intents)][['text', 'intent']]\n",
        "\n",
        "    # Validate dataset\n",
        "    if df_conversational.empty:\n",
        "        raise ValueError(\"No conversational intents found in dataset. Check Cell 2 output.\")\n",
        "\n",
        "    # Check intent distribution\n",
        "    intent_counts = df_conversational['intent'].value_counts()\n",
        "    print('Conversational Intent Distribution:')\n",
        "    for intent in conversational_intents:\n",
        "        count = intent_counts.get(intent, 0)\n",
        "        print(f'{intent}: {count}')\n",
        "        if count < 5:\n",
        "            raise ValueError(f\"Intent {intent} has {count} samples (<5). Add more samples in Cell 2.\")\n",
        "\n",
        "    response_map = {\n",
        "        'greeting': [\n",
        "            'Hello! How can I assist you with your trucking needs today?',\n",
        "            'Hi there! Ready to help with your shipments or account!',\n",
        "            'Good to hear from you! What’s up?'\n",
        "        ],\n",
        "        'farewell': [\n",
        "            'Goodbye! Stay safe on the road.',\n",
        "            'Thanks for connecting!'\n",
        "        ],\n",
        "        'small_talk': [\n",
        "            'My day’s going smoothly, thanks! How’s yours?',\n",
        "            'Trucking life’s always moving! How’s it like for you?',\n",
        "            'Just keeping the wheels turning! Got any big plans?',\n",
        "            'All’s good here! What’s new with you?'\n",
        "        ],\n",
        "        'compliment': [\n",
        "            'Thanks, you’re awesome too! Need help with anything?',\n",
        "            'Appreciate that! What can I do for you today?'\n",
        "        ],\n",
        "        'weather_query': [\n",
        "            'Can’t see the skies, but I can check your route! Where are you headed?',\n",
        "            'Weather’s a mystery here, but I’m ready to help! What’s your destination?'\n",
        "        ]\n",
        "    }\n",
        "\n",
        "    dialogues = []\n",
        "    for _, row in df_conversational.iterrows():\n",
        "        user_input = row['text']\n",
        "        intent = row['intent']\n",
        "        response = random.choice(response_map[intent])\n",
        "        dialogues.append({'input': user_input, 'response': response})\n",
        "\n",
        "    dialogue_df = pd.DataFrame(dialogues)\n",
        "    dataset = Dataset.from_pandas(dialogue_df)\n",
        "\n",
        "    # Manual train/eval split (80/20)\n",
        "    indices = list(range(len(dataset)))\n",
        "    random.shuffle(indices)\n",
        "    split_idx = int(0.8 * len(indices))\n",
        "    train_indices = indices[:split_idx]\n",
        "    eval_indices = indices[split_idx:]\n",
        "    train_dataset = dataset.select(train_indices)\n",
        "    eval_dataset = dataset.select(eval_indices)\n",
        "\n",
        "    os.makedirs('data', exist_ok=True)\n",
        "    dialogue_df.to_csv('data/dialoggpt_dialogues.csv', index=False)\n",
        "\n",
        "    print(f'DialogGPT training data prepared: {len(dialogue_df)} dialogue pairs.')\n",
        "    print(f'Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}')\n",
        "    return train_dataset, eval_dataset\n",
        "\n",
        "dialoggpt_train_dataset, dialoggpt_eval_dataset = prepare_dialoggpt_training_data(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Fine-Tune DialogGPT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b75abf616a94e0a803ed966ec7b9bfd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/43225 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b489dd229198466090dedeae1da23c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/64839 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'loss': 5.7549, 'grad_norm': 72.84285736083984, 'learning_rate': 5e-06, 'epoch': 0.0}\n",
            "{'loss': 3.19, 'grad_norm': 4.131343841552734, 'learning_rate': 1e-05, 'epoch': 0.0}\n",
            "{'loss': 2.3349, 'grad_norm': 2.251282215118408, 'learning_rate': 1.5e-05, 'epoch': 0.0}\n",
            "{'loss': 2.3674, 'grad_norm': 3.2380874156951904, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
            "{'loss': 2.2507, 'grad_norm': 1.827683687210083, 'learning_rate': 2.5e-05, 'epoch': 0.0}\n",
            "{'loss': 1.6018, 'grad_norm': 2.4899799823760986, 'learning_rate': 3e-05, 'epoch': 0.0}\n",
            "{'loss': 1.8352, 'grad_norm': 2.1109228134155273, 'learning_rate': 3.5e-05, 'epoch': 0.0}\n",
            "{'loss': 1.7242, 'grad_norm': 2.2534713745117188, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
            "{'loss': 1.7692, 'grad_norm': 1.7371973991394043, 'learning_rate': 4.5e-05, 'epoch': 0.0}\n",
            "{'loss': 1.2346, 'grad_norm': 1.6723651885986328, 'learning_rate': 5e-05, 'epoch': 0.0}\n",
            "{'loss': 1.4451, 'grad_norm': 42.043540954589844, 'learning_rate': 4.999227668020822e-05, 'epoch': 0.01}\n",
            "{'loss': 1.4559, 'grad_norm': 1.6279628276824951, 'learning_rate': 4.998455336041644e-05, 'epoch': 0.01}\n",
            "{'loss': 1.3665, 'grad_norm': 1.7389039993286133, 'learning_rate': 4.997683004062467e-05, 'epoch': 0.01}\n",
            "{'loss': 1.2133, 'grad_norm': 1.540687084197998, 'learning_rate': 4.9969106720832884e-05, 'epoch': 0.01}\n",
            "{'loss': 1.2493, 'grad_norm': 1.2422568798065186, 'learning_rate': 4.9961383401041104e-05, 'epoch': 0.01}\n",
            "{'loss': 1.2081, 'grad_norm': 2.020453929901123, 'learning_rate': 4.9953660081249324e-05, 'epoch': 0.01}\n",
            "{'loss': 1.0875, 'grad_norm': 9.607577323913574, 'learning_rate': 4.9945936761457545e-05, 'epoch': 0.01}\n",
            "{'loss': 1.4796, 'grad_norm': 1.9724735021591187, 'learning_rate': 4.9938213441665765e-05, 'epoch': 0.01}\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import torch  # Ensure import if not in Cell 1\n",
        "\n",
        "def fine_tune_dialoggpt(train_dataset, eval_dataset):\n",
        "    model_name = 'microsoft/DialoGPT-medium'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "    def preprocess_dialogues(example):\n",
        "        try:\n",
        "            # Process single example with PyTorch tensors\n",
        "            conversation = f\"{example['input']} {tokenizer.eos_token} {example['response']}\"\n",
        "            tokenized = tokenizer(\n",
        "                conversation,\n",
        "                padding='max_length',\n",
        "                truncation=True,\n",
        "                max_length=128,\n",
        "                return_tensors='pt'  # PyTorch tensors\n",
        "            )\n",
        "            result = {\n",
        "                'input_ids': tokenized['input_ids'].squeeze(0),\n",
        "                'attention_mask': tokenized['attention_mask'].squeeze(0),\n",
        "                'labels': tokenized['input_ids'].squeeze(0).clone()\n",
        "            }\n",
        "            return result\n",
        "        except Exception as e:\n",
        "            print(f'Error in preprocess_dialogues: {e}')\n",
        "            raise\n",
        "\n",
        "    try:\n",
        "        # Map without batching\n",
        "        tokenized_train_dataset = train_dataset.map(preprocess_dialogues, remove_columns=['input', 'response'])\n",
        "        tokenized_eval_dataset = eval_dataset.map(preprocess_dialogues, remove_columns=['input', 'response'])\n",
        "        tokenized_train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "        tokenized_eval_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "    except Exception as e:\n",
        "        print(f'Error tokenizing dataset: {e}')\n",
        "        return None, None\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./dialoggpt_results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=1,\n",
        "        per_device_eval_batch_size=1,\n",
        "        warmup_steps=10,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./dialoggpt_logs',\n",
        "        logging_steps=20,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        dataloader_num_workers=0\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_train_dataset,\n",
        "        eval_dataset=tokenized_eval_dataset\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model('./dialoggpt_model')\n",
        "        tokenizer.save_pretrained('./dialoggpt_model')\n",
        "        print('DialogGPT model fine-tuned and saved.')\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f'Error fine-tuning DialogGPT: {e}')\n",
        "        print('Falling back to pretrained DialogGPT model.')\n",
        "        return None, None\n",
        "\n",
        "dialoggpt_model, dialoggpt_tokenizer = fine_tune_dialoggpt(dialoggpt_train_dataset, dialoggpt_eval_dataset)\n",
        "if dialoggpt_model is None:\n",
        "    print('Using pretrained DialogGPT due to fine-tuning error.')\n",
        "    dialoggpt_model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-medium')\n",
        "    dialoggpt_tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-medium')\n",
        "    dialoggpt_tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Preprocess for DistilBERT\n",
        "\n",
        "Ensure all intents are included in the tokenized dataset and label_map.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\nishe\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a057aa5564f4b608116ca36599be572",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/56840 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59df1af242e74a0495fc83cf8da0d1d9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/56840 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Distribution in Dataset:\n",
            " -1    56840\n",
            "Name: count, dtype: int64\n",
            "Label Map: {'account_update': 0, 'billing_issue': 1, 'compliment': 2, 'delivery_status': 3, 'farewell': 4, 'fuel_card_query': 5, 'general_query': 6, 'greeting': 7, 'service_inquiry': 8, 'small_talk': 9, 'weather_query': 10}\n"
          ]
        }
      ],
      "source": [
        "def preprocess_data(df):\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    def tokenize_function(examples):\n",
        "        return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=128)\n",
        "    \n",
        "    # Create label_map with all expected intents\n",
        "    expected_intents = ['delivery_status', 'billing_issue', 'account_update', 'service_inquiry', 'fuel_card_query', 'general_query', 'greeting', 'farewell', 'small_talk', 'compliment', 'weather_query']\n",
        "    label_map = {label: idx for idx, label in enumerate(sorted(expected_intents))}\n",
        "\n",
        "    # Verify 'intent' column exists\n",
        "    if 'intent' not in df.columns:\n",
        "        raise ValueError(\"'intent' column missing in DataFrame. Check Cell 2 output.\")\n",
        "    \n",
        "    # Filter dataset to include only expected intents\n",
        "    df_filtered = df[df['intent'].isin(expected_intents)][['text', 'intent']].copy()\n",
        "\n",
        "    # Check for empty dataset\n",
        "    if df_filtered.empty:\n",
        "        raise ValueError(\"Filtered dataset is empty. Check Cell 2 filtering or intent distribution.\")\n",
        "    \n",
        "    # Check for missing intents and warn\n",
        "    present_intents = df_filtered['intent'].unique()\n",
        "    missing_intents = [intent for intent in expected_intents if intent not in present_intents]\n",
        "    if missing_intents:\n",
        "        print(f'Warning: Intents {missing_intents} missing in dataset after filtering.')\n",
        "\n",
        "    dataset = Dataset.from_pandas(df_filtered)\n",
        "    \n",
        "    # Validate dataset size\n",
        "    if len(dataset) == 0:\n",
        "        raise ValueError(\"Dataset is empty after conversion. Check Cell 2 output.\")\n",
        "\n",
        "    # Compute label distribution\n",
        "    label_counts = pd.Series([label_map.get(x['intent'], -1) for x in dataset]).value_counts()\n",
        "    print('Label Distribution in Dataset (Label ID: Count):')\n",
        "    for label_id, count in label_counts.items():\n",
        "        if label_id == -1:\n",
        "            print(f'Invalid Labels: {count}')\n",
        "        else:\n",
        "            intent = list(label_map.keys())[list(label_map.values()).index(label_id)]\n",
        "            print(f'{intent} ({label_id}): {count}')\n",
        "    \n",
        "    # Check for invalid or insufficient labels\n",
        "    if -1 in label_counts:\n",
        "        raise ValueError(f\"Invalid intents found: {label_counts[-1]} samples not in {expected_intents}.\")\n",
        "    min_samples = 5\n",
        "    low_sample_labels = {label_id: count for label_id, count in label_counts.items() if count < min_samples and label_id != -1}\n",
        "    if low_sample_labels:\n",
        "        low_intents = {list(label_map.keys())[list(label_map.values()).index(k)]: v for k, v in low_sample_labels.items()}\n",
        "        raise ValueError(f\"Intents with < {min_samples} samples: {low_intents}. Add more samples in Cell 2.\")\n",
        "    \n",
        "    # Tokenize and rename 'intent' to 'labels'\n",
        "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "    tokenized_dataset = tokenized_dataset.rename_column('intent', 'labels')\n",
        "    tokenized_dataset = tokenized_dataset.map(lambda x: {'labels': label_map[x['labels']]})\n",
        "    tokenized_dataset = tokenized_dataset.cast_column('labels', ClassLabel(names=list(label_map.keys())))\n",
        "\n",
        "    # Verify dataset features\n",
        "    print('Dataset Features:\\n', tokenized_dataset.features)\n",
        "    return tokenized_dataset, label_map, tokenizer\n",
        "\n",
        "dataset, label_map, tokenizer = preprocess_data(df)\n",
        "print(f'Label Map: {label_map}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train DistilBERT Model\n",
        "\n",
        "Ensure model is trained with correct number of labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(dataset, label_map):\n",
        "    # Get unique labels in dataset\n",
        "    unique_labels = sorted(set(dataset['labels']))\n",
        "    num_labels = len(unique_labels)\n",
        "    print(f'Training with {num_labels} unique labels: {unique_labels}')\n",
        "\n",
        "    # Validate label consistency\n",
        "    expected_num_labels = len(label_map)\n",
        "    if num_labels != expected_num_labels:\n",
        "        print(f'Warning: Dataset has {num_labels} labels, but label_map has {expected_num_labels}.')\n",
        "\n",
        "    model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "\n",
        "    # Manual stratified split using sklearn\n",
        "    try:\n",
        "        # Extract features and labels\n",
        "        indices = list(range(len(dataset)))\n",
        "        labels = [dataset[i]['labels'] for i in indices]\n",
        "        train_indices, eval_indices = train_test_split(\n",
        "            indices,\n",
        "            test_size=0.2,\n",
        "            stratify=labels,\n",
        "            random_state=42\n",
        "        )\n",
        "        train_dataset = dataset.select(train_indices)\n",
        "        eval_dataset = dataset.select(eval_indices)\n",
        "        print(f'Train dataset size: {len(train_dataset)}, Eval dataset size: {len(eval_dataset)}')\n",
        "    except Exception as e:\n",
        "        print(f'Error during dataset split: {e}')\n",
        "        return None, None, None, None, unique_labels\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./results',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=8,\n",
        "        warmup_steps=500,\n",
        "        weight_decay=0.01,\n",
        "        logging_dir='./logs',\n",
        "        logging_steps=10,\n",
        "        eval_strategy='epoch',\n",
        "        save_strategy='epoch',\n",
        "        load_best_model_at_end=True,\n",
        "        fp16=torch.cuda.is_available(),\n",
        "        dataloader_num_workers=0\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=eval_dataset\n",
        "    )\n",
        "    try:\n",
        "        trainer.train()\n",
        "        trainer.save_model('./chatbot_model')\n",
        "        return trainer, model, train_dataset, eval_dataset, unique_labels\n",
        "    except Exception as e:\n",
        "        print(f'Error training DistilBERT model: {e}')\n",
        "        return None, None, None, None, unique_labels  # Return 5 values\n",
        "\n",
        "trainer, model, train_dataset, eval_dataset, unique_labels = train_model(dataset, label_map)\n",
        "if trainer is None:\n",
        "    raise RuntimeError('Failed to train DistilBERT model.')\n",
        "with open('label_map.json', 'w') as f:\n",
        "    json.dump(label_map, f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Evaluate Model\n",
        "\n",
        "Fix class mismatch by specifying labels in classification_report.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(trainer, eval_dataset, label_map, unique_labels):\n",
        "    try:\n",
        "        predictions = trainer.predict(eval_dataset)\n",
        "        preds = np.argmax(predictions.predictions, axis=1)\n",
        "        labels = predictions.label_ids\n",
        "\n",
        "        # Map unique_labels back to intent names\n",
        "        reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "        target_names = [reverse_label_map[label] for label in unique_labels]\n",
        "\n",
        "        accuracy = accuracy_score(labels, preds)\n",
        "        report = classification_report(\n",
        "            labels,\n",
        "            preds,\n",
        "            labels=unique_labels,\n",
        "            target_names=target_names,\n",
        "            zero_division=0\n",
        "        )\n",
        "        print(f'Accuracy: {accuracy:.4f}')\n",
        "        print(f'Classification Report:\\n{report}\\n')\n",
        "\n",
        "        cm = confusion_matrix(labels, preds, labels=unique_labels)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('True')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.show()\n",
        "    except Exception as e:\n",
        "        print(f'Error evaluating model: {e}')\n",
        "\n",
        "evaluate_model(trainer, eval_dataset, label_map, unique_labels)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Dialogue Management\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DialogueManager:\n",
        "    def __init__(self, model, tokenizer, label_map, dialoggpt_model, dialoggpt_tokenizer):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dialoggpt_model = dialoggpt_model\n",
        "        self.dialoggpt_tokenizer = dialoggpt_tokenizer\n",
        "        self.reverse_label_map = {v: k for k, v in label_map.items()}\n",
        "        self.state = 'INITIAL'\n",
        "        self.context = defaultdict(str)\n",
        "        self.history = []\n",
        "        self.fallback_responses = {\n",
        "            'greeting': {\n",
        "                'INITIAL': [\n",
        "                    'Hello! How can I assist you with your trucking needs today?',\n",
        "                    'Hi there! What can I help you with regarding your shipments or account?'\n",
        "                ],\n",
        "                'POST_GREETING': [\n",
        "                    'Thanks for the greeting! How can I assist with your shipment or billing needs?',\n",
        "                    'Nice to connect again! What’s on your mind today?'\n",
        "                ]\n",
        "            },\n",
        "            'farewell': {\n",
        "                'INITIAL': [\n",
        "                    'Goodbye! Feel free to reach out if you need further assistance.',\n",
        "                    'Thanks for connecting! Let me know if you have more questions later.'\n",
        "                ]\n",
        "            },\n",
        "            'small_talk': {\n",
        "                'INITIAL': {\n",
        "                    'mood': ['My day’s going smoothly, thanks for asking! How’s yours?'],\n",
        "                    'plans': ['No big plans here, just helping truckers! Got any big plans yourself?'],\n",
        "                    'industry': ['Trucking life’s always moving! How’s it treating you these days?'],\n",
        "                    'default': ['All’s well here, thanks for asking! Need help with your shipments?']\n",
        "                }\n",
        "            },\n",
        "            'compliment': {\n",
        "                'INITIAL': ['Thank you, that’s kind of you! How can I assist you today?']\n",
        "            },\n",
        "            'weather_query': {\n",
        "                'INITIAL': ['Weather’s clear here, but I can check for your route! Where are you headed?']\n",
        "            }\n",
        "        }\n",
        "        self.trucking_responses = {\n",
        "            'delivery_status': {\n",
        "                'INITIAL': 'I can check your shipment status for {location}. Please provide the shipment ID.',\n",
        "                'AWAITING_SHIPMENT_ID': 'Could you share the shipment ID to proceed with tracking?',\n",
        "                'PROVIDED_SHIPMENT_ID': 'Thank you. Shipment {shipment_id} is currently at {location}. Would you like the estimated arrival time?'\n",
        "            },\n",
        "            'billing_issue': {\n",
        "                'INITIAL': 'Let’s review your billing issue with {company}. Is this about an overcharge or a payment concern?',\n",
        "                'AWAITING_DETAILS': 'Can you provide the invoice number or {company} transaction amount?',\n",
        "                'RESOLVING': 'I’ve noted a {amount} charge on your {company} invoice. Would you like to dispute this?'\n",
        "            },\n",
        "            'account_update': {\n",
        "                'INITIAL': 'I can help update your account details for {regulation}. What information would you like to change?',\n",
        "                'AWAITING_INFO': 'Please provide the new address or contact details for {regulation}.',\n",
        "                'CONFIRMING': 'I have {new_info} for your {regulation} update. Please confirm to proceed.'\n",
        "            },\n",
        "            'service_inquiry': {\n",
        "                'INITIAL': 'I can provide information on our services. Are you interested in flatbed, refrigerated, or bulk transport rates?',\n",
        "                'AWAITING_SPECIFICS': 'Which service are you inquiring about: flatbed, refrigerated, or bulk transport?'\n",
        "            },\n",
        "            'fuel_card_query': {\n",
        "                'INITIAL': 'I can assist with your {company} fuel card or {regulation} query. Is this about a balance, transaction, or compliance?',\n",
        "                'AWAITING_DETAILS': 'Could you specify if this is a {company} card issue or a {regulation} tax question?'\n",
        "            },\n",
        "            'general_query': {\n",
        "                'INITIAL': 'Could you clarify your request? I can help with delivery, billing, account updates, or {company}/{regulation} services.'\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def predict_intent(self, text):\n",
        "        inputs = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "            predicted_label = torch.argmax(outputs.logits, dim=1).item()\n",
        "        return self.reverse_label_map.get(predicted_label, 'general_query')  # Fallback to general_query\n",
        "\n",
        "    def extract_entities(self, text):\n",
        "        entities = []\n",
        "        text = text.lower()\n",
        "        if 'speedway' in text:\n",
        "            entities.append({'entity': 'location', 'value': 'Speedway'})\n",
        "        if 'comdata' in text:\n",
        "            entities.append({'entity': 'company', 'value': 'Comdata'})\n",
        "        if 'ifta' in text:\n",
        "            entities.append({'entity': 'regulation', 'value': 'IFTA'})\n",
        "        shipment_match = re.search(r'\\bship\\d+\\b', text, re.IGNORECASE)\n",
        "        if shipment_match:\n",
        "            entities.append({'entity': 'shipment_id', 'value': shipment_match.group()})\n",
        "        amount_match = re.search(r'\\$\\d+', text)\n",
        "        if amount_match:\n",
        "            entities.append({'entity': 'amount', 'value': amount_match.group()})\n",
        "        if 'new address' in text or 'change to' in text:\n",
        "            new_info = text.split('new address')[-1].strip() or text.split('change to')[-1].strip()\n",
        "            entities.append({'entity': 'new_info', 'value': new_info[:50]})\n",
        "        return entities\n",
        "\n",
        "    def get_dialoggpt_response(self, text):\n",
        "        try:\n",
        "            if self.dialoggpt_tokenizer.pad_token is None:\n",
        "                self.dialoggpt_tokenizer.pad_token = self.dialoggpt_tokenizer.eos_token\n",
        "            input_ids = self.dialoggpt_tokenizer.encode(text + self.dialoggpt_tokenizer.eos_token, return_tensors='pt')\n",
        "            reply_ids = self.dialoggpt_model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=128,\n",
        "                pad_token_id=self.dialoggpt_tokenizer.eos_token_id,\n",
        "                no_repeat_ngram_size=3,\n",
        "                top_p=0.9,\n",
        "                temperature=0.7,\n",
        "                do_sample=True\n",
        "            )\n",
        "            response = self.dialoggpt_tokenizer.decode(reply_ids[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "            return response\n",
        "        except Exception as e:\n",
        "            print(f'Error getting DialogGPT response: {e}')\n",
        "            return None\n",
        "\n",
        "    def update_context(self, intent, entities, text):\n",
        "        self.history.append((intent, text, ''))\n",
        "        for entity in entities:\n",
        "            self.context[entity['entity']] = entity['value']\n",
        "        if intent == 'greeting':\n",
        "            self.context['greeted'] = 'true'\n",
        "        if intent == 'small_talk':\n",
        "            text = text.lower()\n",
        "            if 'day' in text or 'how’s your day' in text:\n",
        "                self.context['small_talk_type'] = 'mood'\n",
        "            elif 'plans' in text or 'big plans' in text:\n",
        "                self.context['small_talk_type'] = 'plans'\n",
        "            elif 'trucking' in text or 'truck' in text:\n",
        "                self.context['small_talk_type'] = 'industry'\n",
        "            else:\n",
        "                self.context['small_talk_type'] = 'default'\n",
        "\n",
        "    def transition_state(self, intent, entities, text):\n",
        "        if intent in ['greeting', 'small_talk', 'compliment', 'weather_query']:\n",
        "            self.state = 'POST_GREETING' if self.state == 'INITIAL' and intent == 'greeting' else self.state\n",
        "        elif intent == 'farewell':\n",
        "            self.state = 'INITIAL'\n",
        "        elif intent == 'delivery_status':\n",
        "            if self.state == 'INITIAL' and 'shipment_id' not in self.context:\n",
        "                self.state = 'AWAITING_SHIPMENT_ID'\n",
        "            elif 'shipment_id' in self.context:\n",
        "                self.state = 'PROVIDED_SHIPMENT_ID'\n",
        "        elif intent == 'billing_issue':\n",
        "            if self.state == 'INITIAL' and not any(e['entity'] in ['amount', 'transaction_id'] for e in entities):\n",
        "                self.state = 'AWAITING_DETAILS'\n",
        "            elif any(e['entity'] in ['amount', 'transaction_id'] for e in entities):\n",
        "                self.state = 'RESOLVING'\n",
        "        elif intent == 'account_update':\n",
        "            if self.state == 'INITIAL' and 'new_info' not in self.context:\n",
        "                self.state = 'AWAITING_INFO'\n",
        "            elif 'new_info' in self.context:\n",
        "                self.state = 'CONFIRMING'\n",
        "        elif intent == 'service_inquiry':\n",
        "            if self.state == 'INITIAL' and 'service_type' not in self.context:\n",
        "                self.state = 'AWAITING_SPECIFICS'\n",
        "        elif intent == 'fuel_card_query':\n",
        "            if self.state == 'INITIAL' and not any(e['entity'] in ['balance', 'transaction'] for e in entities):\n",
        "                self.state = 'AWAITING_DETAILS'\n",
        "\n",
        "    def generate_response(self, intent, entities, text):\n",
        "        self.update_context(intent, entities, text)\n",
        "        self.transition_state(intent, entities, text)\n",
        "\n",
        "        if intent in ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query']:\n",
        "            dialog_response = self.get_dialoggpt_response(text)\n",
        "            if not dialog_response or len(dialog_response) < 5 or any(word in dialog_response.lower() for word in ['inappropriate', 'sorry', 'weird', 'lol']):\n",
        "                small_talk_type = self.context.get('small_talk_type', 'default')\n",
        "                if intent == 'small_talk':\n",
        "                    response_options = self.fallback_responses[intent]['INITIAL'].get(small_talk_type, self.fallback_responses[intent]['INITIAL']['default'])\n",
        "                else:\n",
        "                    response_options = self.fallback_responses.get(intent, {'INITIAL': ['Could you clarify your request?']}).get(self.state, self.fallback_responses[intent]['INITIAL'])\n",
        "                response = random.choice(response_options) if isinstance(response_options, list) else response_options\n",
        "            else:\n",
        "                response = dialog_response + ' Need assistance with your trucking needs?'\n",
        "            if intent == 'greeting' and 'good morning' in text.lower() and datetime.now().hour < 12:\n",
        "                response = random.choice(['Good morning to you too! How can I assist today?', 'Morning! Ready to help with your trucking needs.'])\n",
        "        else:\n",
        "            response_template = self.trucking_responses.get(intent, self.trucking_responses['general_query']).get(self.state, self.trucking_responses[intent]['INITIAL'])\n",
        "            try:\n",
        "                response = response_template.format(\n",
        "                    location=self.context.get('location', 'unknown'),\n",
        "                    company=self.context.get('company', 'unknown'),\n",
        "                    regulation=self.context.get('regulation', 'unknown'),\n",
        "                    shipment_id=self.context.get('shipment_id', 'unknown'),\n",
        "                    amount=self.context.get('amount', 'unknown'),\n",
        "                    new_info=self.context.get('new_info', 'unknown')\n",
        "                )\n",
        "            except KeyError:\n",
        "                response = response_template\n",
        "\n",
        "        if self.context.get('greeted') == 'true' and intent not in ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query'] and 'PROVIDED' in self.state:\n",
        "            response = f'Since you greeted me earlier, I’m ready to assist! {response}'\n",
        "\n",
        "        self.history[-1] = (intent, text, response)\n",
        "        resolved = self.state in ['PROVIDED_SHIPMENT_ID', 'RESOLVING', 'CONFIRMING'] and intent not in ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query']\n",
        "        return response, resolved\n",
        "\n",
        "    def evaluate_dialogue_success(self):\n",
        "        resolved = sum(1 for intent, _, _ in self.history if intent not in ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query'] and self.state in ['PROVIDED_SHIPMENT_ID', 'RESOLVING', 'CONFIRMING'])\n",
        "        total = sum(1 for intent, _, _ in self.history if intent not in ['greeting', 'farewell', 'small_talk', 'compliment', 'weather_query'])\n",
        "        return resolved / total if total > 0 else 0.0\n",
        "\n",
        "try:\n",
        "    with open('label_map.json', 'r') as f:\n",
        "        label_map = json.load(f)\n",
        "    dialogue_manager = DialogueManager(model, tokenizer, label_map, dialoggpt_model, dialoggpt_tokenizer)\n",
        "except FileNotFoundError:\n",
        "    print('Error: label_map.json not found. Please ensure Cell 6 has run successfully.')\n",
        "    dialogue_manager = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Interactive Chatbot UI\n",
        "\n",
        "UI with DialogGPT conversational support trained on Twitter data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if dialogue_manager is None:\n",
        "    print('Dialogue manager not initialized. Please fix previous errors.')\n",
        "else:\n",
        "    # Create UI\n",
        "    input_box = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Type your query (e.g., Hello, How’s your day?, or Where’s my shipment?)',\n",
        "        description='Query:',\n",
        "        layout={'width': '500px'}\n",
        "    )\n",
        "    submit_button = widgets.Button(\n",
        "        description='Submit',\n",
        "        button_style='success',\n",
        "        tooltip='Submit query'\n",
        "    )\n",
        "    follow_up_button1 = widgets.Button(\n",
        "        description='Request ETA',\n",
        "        button_style='info',\n",
        "        tooltip='Request ETA',\n",
        "        layout={'visibility': 'hidden'}\n",
        "    )\n",
        "    follow_up_button2 = widgets.Button(\n",
        "        description='Confirm',\n",
        "        button_style='success',\n",
        "        tooltip='Confirm action',\n",
        "        layout={'visibility': 'hidden'}\n",
        "    )\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    def on_submit_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            user_input = input_box.value.strip()\n",
        "            if not user_input:\n",
        "                print('Please enter a query.')\n",
        "                return\n",
        "            print(f'You: {user_input}')\n",
        "            try:\n",
        "                intent = dialogue_manager.predict_intent(user_input)\n",
        "                entities = dialogue_manager.extract_entities(user_input)\n",
        "                response, resolved = dialogue_manager.generate_response(intent, entities, user_input)\n",
        "                print(f'Bot: {response}')\n",
        "                input_box.value = ''\n",
        "                follow_up_button1.layout.visibility = 'visible' if dialogue_manager.state == 'PROVIDED_SHIPMENT_ID' else 'hidden'\n",
        "                follow_up_button2.layout.visibility = 'visible' if dialogue_manager.state in ['RESOLVING', 'CONFIRMING'] else 'hidden'\n",
        "                success_rate = dialogue_manager.evaluate_dialogue_success()\n",
        "                if success_rate > 0:\n",
        "                    print(f'Dialogue Success Rate: {success_rate:.2f}')\n",
        "            except Exception as e:\n",
        "                print(f'Error processing query: {e}')\n",
        "\n",
        "    def on_follow_up1_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            response = f'The estimated arrival time for shipment {dialogue_manager.context.get(\"shipment_id\", \"unknown\")} is tomorrow by 3 PM.'\n",
        "            dialogue_manager.history.append(('follow_up', 'Request ETA', response))\n",
        "            print(f'Bot: {response}')\n",
        "            follow_up_button1.layout.visibility = 'hidden'\n",
        "            success_rate = dialogue_manager.evaluate_dialogue_success()\n",
        "            if success_rate > 0:\n",
        "                print(f'Dialogue Success Rate: {success_rate:.2f}')\n",
        "\n",
        "    def on_follow_up2_clicked(b):\n",
        "        with output_area:\n",
        "            clear_output()\n",
        "            if dialogue_manager.state == 'CONFIRMING':\n",
        "                response = f'Confirmed. {dialogue_manager.context.get(\"new_info\", \"action\")} has been updated.'\n",
        "            else:\n",
        "                response = f'Dispute for {dialogue_manager.context.get(\"amount\", \"unknown\")} has been submitted.'\n",
        "            dialogue_manager.history.append(('follow_up', 'Confirm', response))\n",
        "            dialogue_manager.state = 'INITIAL'\n",
        "            print(f'Bot: {response}')\n",
        "            follow_up_button2.layout.visibility = 'hidden'\n",
        "            success_rate = dialogue_manager.evaluate_dialogue_success()\n",
        "            if success_rate > 0:\n",
        "                print(f'Dialogue Success Rate: {success_rate:.2f}')\n",
        "\n",
        "    submit_button.on_click(on_submit_clicked)\n",
        "    follow_up_button1.on_click(on_follow_up1_clicked)\n",
        "    follow_up_button2.on_click(on_follow_up2_clicked)\n",
        "\n",
        "    # Display UI\n",
        "    display(widgets.VBox([\n",
        "        widgets.HTML('<h3>Trucking Co. Customer Chatbot</h3>'),\n",
        "        input_box,\n",
        "        submit_button,\n",
        "        follow_up_button1,\n",
        "        follow_up_button2,\n",
        "        output_area\n",
        "    ]))\n",
        "    print('Welcome! You can greet me, chat about your day, or ask about delivery, billing, or Comdata/IFTA services.')\n",
        "    print('Type your query and click Submit.')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8",
      "language": "python",
      "name": "python3.8"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
